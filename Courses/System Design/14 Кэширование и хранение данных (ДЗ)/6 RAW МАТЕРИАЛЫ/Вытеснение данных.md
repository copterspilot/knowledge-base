### **Алгоритмы вытеснения (эвикции) данных в кэше**

Алгоритмы вытеснения определяют, **какие данные удалять из кэша** при нехватке места. Выбор алгоритма критично влияет на производительность и hit ratio.

---

## **1. Основные алгоритмы**

### **1. LRU (Least Recently Used)**
**Принцип:**  
Удаляет данные, к которым **дольше всего не обращались**.  

**Как работает:**  
- Каждый доступ к элементу обновляет его позицию в очереди.  
- При переполнении удаляется «самый старый» элемент.  

**Плюсы:**  
- Простота реализации.  
- Эффективен для большинства сценариев.  

**Минусы:**  
- Не учитывает частоту обращений (может вытеснить «важный» ключ, к которому редко обращались в последнее время).  

**Пример:**  
```python
from functools import lru_cache

@lru_cache(maxsize=100)  # LRU-кэш на 100 элементов
def get_data(key):
    return expensive_operation(key)
```

---

### **2. LFU (Least Frequently Used)**
**Принцип:**  
Удаляет данные с **наименьшей частотой обращений**.  

**Как работает:**  
- Для каждого элемента ведётся счётчик запросов.  
- При переполнении удаляется ключ с минимальным счётчиком.  

**Плюсы:**  
- Подходит для long-tail данных (редкие, но важные запросы).  

**Минусы:**  
- Дорогой подсчёт частот.  
- «Забывает» старые паттерны доступа (не адаптируется к изменениям).  

**Пример:**  
```python
# LFU реализуется через специализированные библиотеки (например, cachetools)
from cachetools import LFUCache
cache = LFUCache(maxsize=100)
```

---

### **3. FIFO (First In, First Out)**
**Принцип:**  
Удаляет данные в порядке их добавления («первым пришёл — первым ушёл»).  

**Как работает:**  
- Элементы добавляются в очередь.  
- При переполнении удаляется самый «старый» элемент.  

**Плюсы:**  
- Простота и низкие накладные расходы.  

**Минусы:**  
- Может удалять часто используемые данные, добавленные раньше других.  

**Пример:**  
```python
from cachetools import FIFOCache
cache = FIFOCache(maxsize=100)
```

---

### **4. LIRS (Low Inter-reference Recency Set)**
**Принцип:**  
Комбинирует LRU и LFU, разделяя кэш на «горячие» и «холодные» данные.  

**Как работает:**  
- Использует две очереди:  
  - **HIR** (High Inter-reference Recency) — редко используемые данные.  
  - **LIR** (Low Inter-reference Recency) — часто используемые.  
- При переполнении удаляет из HIR.  

**Плюсы:**  
- Высокий hit ratio в сложных нагрузках.  

**Минусы:**  
- Сложная реализация.  

**Где используется:**  
- Базы данных (Oracle, MySQL InnoDB).  

---

### **5. ARC (Adaptive Replacement Cache)**
**Принцип:**  
Автоматически адаптируется к паттернам доступа, балансируя между LRU и LFU.  

**Как работает:**  
- Ведёт две очереди:  
  - **T1** (недавно использованные).  
  - **T2** (часто использованные).  
- Динамически изменяет их размер.  

**Плюсы:**  
- Лучшая производительность, чем у LRU/LFU.  

**Минусы:**  
- Высокое потребление памяти.  

**Где используется:**  
- СУБД (IBM DB2, ZFS).  

---

### **6. Random Replacement (RR)**
**Принцип:**  
Удаляет **случайный** элемент.  

**Как работает:**  
- При переполнении выбирает ключ для удаления случайно.  

**Плюсы:**  
- Минимальные накладные расходы.  

**Минусы:**  
- Низкий hit ratio.  

**Где используется:**  
- Экстремально high-load системы, где важна скорость, а не точность.  

---

## **2. Сравнение алгоритмов**

| Алгоритм | Сложность | Hit Ratio | Память | Использование               |
|----------|-----------|-----------|--------|-----------------------------|
| **LRU**  | O(1)      | Высокий   | Низкая | Универсальные системы       |
| **LFU**  | O(1)–O(n) | Очень высокий | Высокая | Long-tail данные (CDN)      |
| **FIFO** | O(1)      | Низкий    | Низкая | Простые кэши                |
| **LIRS** | O(1)      | Очень высокий | Высокая | СУБД, файловые системы      |
| **ARC**  | O(1)      | Максимальный | Высокая | Критичные к производительности системы |
| **RR**   | O(1)      | Низкий    | Низкая | High-load, когда скорость > точности |

---

## **3. Как выбрать алгоритм?**
1. **Для общего кэширования** → LRU.  
2. **Для данных с неравномерной нагрузкой** → LFU или ARC.  
3. **Для максимальной скорости** → FIFO или RR.  
4. **Для сложных паттернов доступа** → LIRS/ARC.  

---

## **4. Проверочные вопросы**

### **Вопрос 1**  
**Какой алгоритм вытеснит ключ, к которому не обращались последние 10 минут, но ранее использовали 1000 раз?**  
- а) **LRU**  
- б) LFU  
- в) FIFO  

**Ответ:** а) *LRU учитывает только время последнего доступа.*

---

### **Вопрос 2**  
**Почему LFU не используют в Redis по умолчанию?**  
- а) **Требует хранения счётчиков частот, что дорого по памяти.**  
- б) Не поддерживается протоколом RESP.  
- в) У него низкий hit ratio.  

**Ответ:** а) *LFU сложнее реализовать эффективно.*

---

### **Вопрос 3**  
**Какой алгоритм лучше для кэширования новостной ленты, где 1% запросов — к старым статьям?**  
- а) FIFO  
- б) **LFU**  
- в) RR  

**Ответ:** б) *LFU сохранит «старые, но важные» статьи.*

---

### **Вопрос 4**  
**Что удалит ARC при переполнении?**  
- а) Самый старый ключ.  
- б) **Ключ из «холодной» очереди (HIR).**  
- в) Случайный ключ.  

**Ответ:** б) *ARC динамически управляет двумя очередями (LIR/HIR).*

---

## **5. Вывод**  
- **LRU** — лучший выбор «по умолчанию».  
- **LFU/ARC** — для сложных нагрузок с long-tail.  
- **FIFO/RR** — для минимальных накладных расходов.  

Правильный алгоритм повышает hit ratio на **10–30%** в высоконагруженных системах.
